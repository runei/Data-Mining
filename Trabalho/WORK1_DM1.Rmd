---
title: "WORK1_DM1"
author: "HSO"
date: "November 24, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Practical Assigment Data Minning
#### The folloing work is devided into 3 maisn parts:
# 1. Data Cleaning and transformation
# 2. Data Sumarization and analisys
# 3. Prediction


## Part 1

#####1.1- Reading the xls file, with "-" as non an Value.
##### Initialy it tries to guess the columns types. 
```{r, message=FALSE}
library("readxl")
library(dplyr)
```
```{r }
myData <- read_excel("crime.xls", sheet=1, col_names = TRUE, col_types = NULL, na = "-")
head(myData)
```

#####1.2- Check if variable is a data frame format.
```{r}
(is.data.frame(myData))
```

#####1.3- Verify if the columns type are in the correct format.
```{r}
sapply(myData, class)
#Or using extra goddies such as levels
str(myData)
```

#####1.4- Date and Hour are in the incorrect column type. Convert column type.
```{r}
myData$Date <- as.Date( myData$Date, '%Y/%m/%d') #Convert as Date
myData$Hour <- as.numeric(myData$Hour) #Converto to numeric 
```

#####1.5- The column names contain messy caracther that dificult use. Replacing messy columns name with something more simplier.
```{r}
colnames(myData)
```
```{r}
names(myData) <-gsub("'", "", names(myData), fixed=TRUE)
names(myData) <-gsub("#", "n", names(myData), fixed=TRUE)
names(myData) <-gsub(" ", "_", names(myData), fixed=TRUE)
```
```{r}
colnames(myData)
```

#####1.6- Let´s analise the missing cases and witch colums they appear.
```{r}
colSums(is.na(myData)) #Using vectorization
apply(myData, 2, function(x) length(which(is.na(x)))) #Using apply
```

#####1.7- Quick analisys into the number of cases.
```{r}
(nrow(myData))
```

#####1.8- Checking the window time frame of the dataset.
```{r}
firstOcc <- min(myData$Date)
lastOcc <- max(myData$Date)
(timeframe <- lastOcc-firstOcc)
```



## PART 2 - Sumarization

#####2.1- Whats the average offences per hour in descent order?  
```{r}
group_by(myData, Hour) %>% 
        summarize(meanOff= mean(n_offenses, na.rm=TRUE)) %>%
        arrange(desc(meanOff)) 

```

#####2.2- Witch  time of the day (hour) occurs more offences?
```{r}
group_by(myData, Hour) %>%
        summarize(counts= sum(n_offenses, na.rm=TRUE)) %>%
        arrange(desc(counts)) %>%
        slice(1) #Get the peak
```

#####2.3- In witch areas(Beats) occur more offences? Order descent
```{r}
group_by(myData, Beat) %>% 
        summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
        arrange(desc(counts))
```

#####2.4- In Wicht areas(Beats) occurs more offenses?
```{r}
group_by(myData, Beat) %>%
        summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
        arrange(desc(counts)) %>% 
        slice(1)
```
 
#####2.5- What genre of offences are more frequent in the city? 
```{r, message=FALSE}
library(ggplot2)
```
```{r}
ggplot(myData, aes(x= myData$Offense_Type)) +
         geom_bar()+
         ggtitle("Number of offences per type")+
         ylab("# occurences")+
         xlab("Beat Code")
```

#####2.5- What genre of offences occur more often in the city?
```{r}
group_by(myData, Offense_Type) %>%
    summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
    arrange(desc(counts))

```

#####2.6(TODO)- What genre of offenses occur more often per beat?
```{r}
ggplot(myData, aes(x= myData$Beat, weights=n_offenses)) +
         geom_bar()+
         ggtitle("Number of offences per beat")
                
```
#####2.6(TODO)- What genre of offenses occur more often per beat? (As table)
```{r}
myData %>%
  group_by(Offense_Type,Beat)%>%
  summarise(counts=sum(n_offenses)) %>%
  arrange(desc(counts))

```

#####2.7- What Are the more common promises?
```{r}
group_by(myData, Premise) %>%
    summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
    arrange(desc(counts))
  
```

#####2.8- Show weekdays were occcur more offenses.
```{r}
myDataDummy <-myData #Make local copy
myDataDummy$Week_Day <- wday(myDataDummy$Date)# Get Weekday

group_by(myDataDummy, Week_Day) %>%
    summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
    arrange(desc(counts))
#Or
myDataWeek <-myData # dummy copy
myDataWeek$day_of_week <- format(myDataWeek$Date, "%A")

group_by(myDataWeek, day_of_week) %>%
  summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
  arrange(desc(counts))

```

#####2.9- Show the Week day were occurs more offenses.
```{r}
group_by(myDataWeek, day_of_week) %>%
  summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
  arrange(desc(counts)) %>%
  slice(1)
```


#####2.10- Show weekdays were occcur more offenses. (Histogram)
```{r}
ggplot(myDataWeek,aes(x=day_of_week,weights=n_offenses))+
  geom_bar() +
  ggtitle('Histogram of offences by Week Day')

## Using day code instead of day name to be mor visible
myDataDummy <-myData #Make local copy
myDataDummy$Week_Day <- wday(myDataDummy$Date)# Get Weekday

ggplot(myDataDummy,aes(x=Week_Day,weights=n_offenses))+
  geom_bar() +
  ggtitle('Histogram of offences by Week Day')

```


#####2.11- What genre of offenses occur more often per day period.
```{r}
byPeriod <- myData
byPeriod$Day_Period[byPeriod$Hour>=8 &
                        byPeriod$Hour <12]<- "Morning"
byPeriod$Day_Period[byPeriod$Hour>=12 &
                        byPeriod$Hour <19]<- "Afternoon"
byPeriod$Day_Period[(byPeriod$Hour>=19 &
                         byPeriod$Hour <=23) |
                        (byPeriod$Hour>=0 &
                           byPeriod$Hour <8)]<- "Night"

group_by(byPeriod, Day_Period) %>%
    summarise(counts= sum(n_offenses, na.rm=TRUE)) %>%
    arrange(desc(counts))

```

#####2.11(1)- What genre of offenses occur more often per day period. (Histogram).
```{r}
ggplot(byPeriod,aes(x=Day_Period,weights=n_offenses))+
  geom_bar() +
  ggtitle('Histogram of offences by Day Period')
```


## MOre Asks to do

##### Show evolution of offences per mothn

##### show per yaer beatcode
##### witch year occurs more more offenses
##### with month occurs more offences





## PART 3 - Make Predictions

#####3.1- Transform the data if not in ymd format and make dummy copy. 
```{r, message=FALSE}
library(lubridate) # Loading library
```

```{r}
myData <- transform(myData, Date = ymd(Date)) 
myDataCopy <- myData #Dummy copy
```

#####3.2- Creating a new column with day period ("Morning","Afternoon","Night").
```{r}
myDataCopy$Day_Period[myDataCopy$Hour>=8 & myDataCopy$Hour <12]<- "Morning"
myDataCopy$Day_Period[myDataCopy$Hour>=12 & myDataCopy$Hour <19]<- "Afternoon"
myDataCopy$Day_Period[(myDataCopy$Hour>=19 & myDataCopy$Hour <=23) |
                        (myDataCopy$Hour>=0 & myDataCopy$Hour <8)]<- "Night"
```

#####3.3- Create column with day of the week.
```{r}
myDataCopy$Week_Day <- wday(myDataCopy$Date)
```

#####3.4- There is an 1.0000 levels messing around. Removing.
```{r}
is.na(myDataCopy$Offense_Type) <- myDataCopy$Offense_Type == "1.000000"
myDataCopy$Offense_Type <- factor(myDataCopy$Offense_Type)
```

#####3.5- Check the number of cases were "1.0000" was appearing and remove the row.
```{r}
(sum(is.na(myDataCopy$Offense_Type)))#Get number of NA in the column

myDataCopy<-myDataCopy[!is.na(myDataCopy$Offense_Type),] #Remove then 
```

#####3.5- Get Beat Codes, weekdays and day period vector.
```{r}
beatCodesVector <- unique(myDataCopy$Beat)
weekDaysVector <- unique(myDataCopy$Week_Day)
dayPeriodsVector <- unique(myDataCopy$Day_Period)
```


#####3.6- Select only the columns that are relevant for training.
```{r}
setTrainData <-subset(myDataCopy, select=c("Date", "Hour",
                                      "Offense_Type",
                                      "Beat",
                                      "n_offenses",
                                      "Day_Period",
                                      "Week_Day"))
```

#####3.7- Create an Training and Test set.
```{r}
splitVal <- 0.7#Percentage 
# Extract samples
set.seed(1234) # For Reproductivity
idx.tr <-sample(1:nrow(setTrainData),
                as.integer(splitVal*nrow(setTrainData)))
tr <- setTrainData[idx.tr, ] #Training Set
ts <- setTrainData[-idx.tr,] #Test Set
```


#####3.8(1)- Train using Trees.
```{r}
######## 1- Using Trees ##############
######## 1- Using Trees ##############
library(DMwR)
library(rpart.plot)
model_AC <- rpartXse(n_offenses ~ .,tr)
preds <- predict(model_AC,ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)) )

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )
######### END - Trees ############
```

#####3.8(2)- Train using Naive Bayes
```{r}
######## 2- Using Naive Bayes ###########
library(e1071)
model_NB <- naiveBayes(n_offenses ~ ., tr)
preds <- predict(model_NB, ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )

######## END - Naive Bayes ###########
```

#####3.8(3)- Train using kNN
```{r}
######## 3- Using kNN ############
library(class)
library(DMwR)
model_KNN <-kNN(n_offenses ~ .,tr,ts,k=3,norm=TRUE)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )

######## END - kNN ###########
```

#####3.8(4)- Train using SVM
```{r}
######## 4- Using SVM ############
library(e1071)
model_SVM <- svm(n_offenses ~., tr, cost =10, epsilon=0.02)
preds <- predict(model_SVM,ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2) )

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)) ) 

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses))
######## END - SVM ###########
```

#####3.8(5)- Train using NN
```{r}
######## 5- Using NN ############
library(nnet)
model_NN <- nnet(n_offenses ~., tr, size=5, decay=0.1, maxit=1000)
preds <- predict(model_NN,ts, type='class')

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )
######## END - NN ###########
```

#####3.8(6)- Train using MARS
```{r}
######## 6- Using MARS ############
model_MARS <- earth(n_offenses ~., tr)
preds <- predict(model_MARS, ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2))

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))))

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses))
######## END - MARS ###########
```

#####3.8(7)- Train using Simple Bagging
```{r}
######## 7- Using Simple bagging ############
simpleBagging <- function(form,data,model='rpartXse',nModels=100,...) {
  ms <- list()
  n <- nrow(data)
  for(i in 1:nModels) {
    tr <- sample(n,n,replace=T)
    ms[[i]] <- do.call(model,c(list(form,data[tr,]),...))
  }
  ms
}

predict.simpleBagging <- function(models,test) {
  ps <- sapply(models,function(m) predict(m,test))
  apply(ps,1,mean)
}

library(DMwR)
model_BAG <- simpleBagging(n_offenses ~., tr, nModels=300, se=0.5)
preds <- predict.simpleBagging(model_BAG, ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )
######## END - Simple Bagging ###########
```

#####3.8(8)- Train using Advance Bagging
```{r}
######## 8- Using Advance bagging ############
library(DMwR)
model_ADBAG <- bagging(n_offenses ~ ., tr,mfinal=50)
preds <- predict(model_ADBAG, ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )
######## END - Advance Bagging ###########
```

#####3.8(9)- Train using Random Forest
```{r}
######## 9- Using Random forest ############
library(randomForest)

model_RF <- randomForest(medv ~ ., tr)
preds <- predict(model_RF,ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses))
######## END - Random forest ###########
```

#####3.8(10)- Train using Boosting
```{r}
######## 10- Using Boosting ############
model_BOOST <- boosting(n_offenses ~ ., tr)
preds <- predict(model_BOOST, ts)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )
######## END - Using Boosting ###########
```

#####3.8(10)- Train using AdaBoostRegression
```{r}
######## 11- Using AdaBoostRegression ############
model_GBM <- gbm(n_offenses ~ .,distribution='gaussian',data=tr,
         n.trees=20000,verbose=F)
preds <- predict(model_GBM, ts, type='response', n.trees=20000)

### Error Analysis ###
## Mean Square error
(mse <- mean((preds -ts$n_offenses)^2))

## Mean Absolute error
(mae <- mean(abs( preds - tr$n_offenses)))

## Normalized Mean Square error
(nmse <- sum((preds - ts$n_offenses)^2) /
  sum((ts$n_offenses-mean(ts$n_offenses))^2) )

## Normalized Mean Absolute Error
(nmae <- sum(abs(preds - ts$n_offenses)) /
  sum(abs(ts$n_offenses-mean(ts$n_offenses))) )

## Mean Average Percentage Error
(mape <- mean(abs(preds-ts$n_offenses)/ts$n_offenses) )
######## END - AdaBoostRegression ###########
```

